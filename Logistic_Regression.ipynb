{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of training set is: 60000\n",
      "size of testing set is: 10000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# define hyperparameters\n",
    "batch_size = 32\n",
    "learning_rate = 1e-3\n",
    "num_epochs = 100\n",
    "\n",
    "# download MNIST dataset\n",
    "train_dataset = datasets.FashionMNIST(\n",
    "    root='./datasets', train=True, transform=transforms.ToTensor(), download=True)\n",
    "\n",
    "test_dataset = datasets.FashionMNIST(\n",
    "    root='./datasets', train=False, transform=transforms.ToTensor())\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "print(f'size of training set is: {len(train_loader.dataset)}')\n",
    "print(f'size of testing set is: {len(test_loader.dataset)}')\n",
    "\n",
    "# define Logistic Regression model\n",
    "class logisticRegression(nn.Module):\n",
    "    def __init__(self, in_dim, n_class):\n",
    "        super(logisticRegression, self).__init__()\n",
    "        self.logistic = nn.Linear(in_dim, n_class)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.logistic(x)\n",
    "        \n",
    "model = logisticRegression(28 * 28, 10)  # image size is 28x28\n",
    "use_gpu = torch.cuda.is_available()\n",
    "if use_gpu:\n",
    "    model = model.cuda()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/100] Loss: 2.065855, Acc: 0.364792\n",
      "[1/100] Loss: 1.870612, Acc: 0.497344\n",
      "[1/100] Loss: 1.733103, Acc: 0.549306\n",
      "[1/100] Loss: 1.629839, Acc: 0.576120\n",
      "[1/100] Loss: 1.547338, Acc: 0.593146\n",
      "[1/100] Loss: 1.478901, Acc: 0.605747\n",
      "Finish 1 epochs, Loss: 1.463947, Acc: 0.608367\n",
      "Test Loss: 1.104808, Acc: 0.672424\n",
      "[2/100] Loss: 1.076132, Acc: 0.675625\n",
      "[2/100] Loss: 1.052627, Acc: 0.678750\n",
      "[2/100] Loss: 1.032191, Acc: 0.685243\n",
      "[2/100] Loss: 1.012861, Acc: 0.691875\n",
      "[2/100] Loss: 0.997914, Acc: 0.695687\n",
      "[2/100] Loss: 0.983335, Acc: 0.699149\n",
      "Finish 2 epochs, Loss: 0.979783, Acc: 0.700100\n",
      "Test Loss: 0.913353, Acc: 0.707169\n",
      "[3/100] Loss: 0.888202, Acc: 0.723229\n",
      "[3/100] Loss: 0.877912, Acc: 0.726719\n",
      "[3/100] Loss: 0.871046, Acc: 0.729688\n",
      "[3/100] Loss: 0.864936, Acc: 0.730365\n",
      "[3/100] Loss: 0.858127, Acc: 0.732792\n",
      "[3/100] Loss: 0.852726, Acc: 0.734097\n",
      "Finish 3 epochs, Loss: 0.851516, Acc: 0.734817\n",
      "Test Loss: 0.828172, Acc: 0.732428\n",
      "[4/100] Loss: 0.813202, Acc: 0.741771\n",
      "[4/100] Loss: 0.809363, Acc: 0.744844\n",
      "[4/100] Loss: 0.801440, Acc: 0.748785\n",
      "[4/100] Loss: 0.797528, Acc: 0.750234\n",
      "[4/100] Loss: 0.791335, Acc: 0.752229\n",
      "[4/100] Loss: 0.784692, Acc: 0.754479\n",
      "Finish 4 epochs, Loss: 0.784346, Acc: 0.754717\n",
      "Test Loss: 0.776775, Acc: 0.746705\n",
      "[5/100] Loss: 0.760010, Acc: 0.761042\n",
      "[5/100] Loss: 0.756283, Acc: 0.763125\n",
      "[5/100] Loss: 0.751704, Acc: 0.762986\n",
      "[5/100] Loss: 0.747317, Acc: 0.765833\n",
      "[5/100] Loss: 0.742717, Acc: 0.767375\n",
      "[5/100] Loss: 0.741324, Acc: 0.767153\n",
      "Finish 5 epochs, Loss: 0.740474, Acc: 0.767367\n",
      "Test Loss: 0.740997, Acc: 0.757987\n",
      "[6/100] Loss: 0.728978, Acc: 0.768854\n",
      "[6/100] Loss: 0.722773, Acc: 0.772344\n",
      "[6/100] Loss: 0.716139, Acc: 0.774688\n",
      "[6/100] Loss: 0.713996, Acc: 0.775365\n",
      "[6/100] Loss: 0.712227, Acc: 0.776375\n",
      "[6/100] Loss: 0.708894, Acc: 0.777431\n",
      "Finish 6 epochs, Loss: 0.708612, Acc: 0.777683\n",
      "Test Loss: 0.713877, Acc: 0.764377\n",
      "[7/100] Loss: 0.692589, Acc: 0.781354\n",
      "[7/100] Loss: 0.688277, Acc: 0.781927\n",
      "[7/100] Loss: 0.689708, Acc: 0.782014\n",
      "[7/100] Loss: 0.685755, Acc: 0.784063\n",
      "[7/100] Loss: 0.685553, Acc: 0.784583\n",
      "[7/100] Loss: 0.683281, Acc: 0.785000\n",
      "Finish 7 epochs, Loss: 0.684048, Acc: 0.784800\n",
      "Test Loss: 0.692371, Acc: 0.772764\n",
      "[8/100] Loss: 0.679487, Acc: 0.784063\n",
      "[8/100] Loss: 0.669506, Acc: 0.787604\n",
      "[8/100] Loss: 0.669340, Acc: 0.788542\n",
      "[8/100] Loss: 0.665667, Acc: 0.790599\n",
      "[8/100] Loss: 0.663467, Acc: 0.789604\n",
      "[8/100] Loss: 0.664381, Acc: 0.789948\n",
      "Finish 8 epochs, Loss: 0.664109, Acc: 0.790267\n",
      "Test Loss: 0.675165, Acc: 0.777057\n",
      "[9/100] Loss: 0.652108, Acc: 0.792604\n",
      "[9/100] Loss: 0.650372, Acc: 0.794635\n",
      "[9/100] Loss: 0.652672, Acc: 0.795069\n",
      "[9/100] Loss: 0.651903, Acc: 0.794453\n",
      "[9/100] Loss: 0.651693, Acc: 0.794208\n",
      "[9/100] Loss: 0.648776, Acc: 0.794948\n",
      "Finish 9 epochs, Loss: 0.647731, Acc: 0.795233\n",
      "Test Loss: 0.660414, Acc: 0.784345\n",
      "[10/100] Loss: 0.629845, Acc: 0.802188\n",
      "[10/100] Loss: 0.631818, Acc: 0.802240\n",
      "[10/100] Loss: 0.634002, Acc: 0.801458\n",
      "[10/100] Loss: 0.634063, Acc: 0.801094\n",
      "[10/100] Loss: 0.631913, Acc: 0.800563\n",
      "[10/100] Loss: 0.633069, Acc: 0.799670\n",
      "Finish 10 epochs, Loss: 0.633664, Acc: 0.798933\n",
      "Test Loss: 0.648696, Acc: 0.786542\n",
      "[11/100] Loss: 0.626636, Acc: 0.798854\n",
      "[11/100] Loss: 0.626067, Acc: 0.799792\n",
      "[11/100] Loss: 0.625790, Acc: 0.801424\n",
      "[11/100] Loss: 0.623656, Acc: 0.802135\n",
      "[11/100] Loss: 0.622880, Acc: 0.802125\n",
      "[11/100] Loss: 0.622129, Acc: 0.802118\n",
      "Finish 11 epochs, Loss: 0.621743, Acc: 0.802133\n",
      "Test Loss: 0.637504, Acc: 0.789437\n",
      "[12/100] Loss: 0.615360, Acc: 0.805313\n",
      "[12/100] Loss: 0.616218, Acc: 0.803438\n",
      "[12/100] Loss: 0.613557, Acc: 0.804653\n",
      "[12/100] Loss: 0.612973, Acc: 0.804714\n",
      "[12/100] Loss: 0.611157, Acc: 0.805687\n",
      "[12/100] Loss: 0.611745, Acc: 0.804514\n",
      "Finish 12 epochs, Loss: 0.611198, Acc: 0.804300\n",
      "Test Loss: 0.627773, Acc: 0.792831\n",
      "[13/100] Loss: 0.605874, Acc: 0.806146\n",
      "[13/100] Loss: 0.605498, Acc: 0.806354\n",
      "[13/100] Loss: 0.604499, Acc: 0.807083\n",
      "[13/100] Loss: 0.602849, Acc: 0.807396\n",
      "[13/100] Loss: 0.602152, Acc: 0.806979\n",
      "[13/100] Loss: 0.602227, Acc: 0.806754\n",
      "Finish 13 epochs, Loss: 0.601789, Acc: 0.806883\n",
      "Test Loss: 0.619376, Acc: 0.794529\n",
      "[14/100] Loss: 0.587050, Acc: 0.812292\n",
      "[14/100] Loss: 0.597049, Acc: 0.807760\n",
      "[14/100] Loss: 0.594073, Acc: 0.809826\n",
      "[14/100] Loss: 0.594505, Acc: 0.808438\n",
      "[14/100] Loss: 0.595081, Acc: 0.808708\n",
      "[14/100] Loss: 0.593785, Acc: 0.809340\n",
      "Finish 14 epochs, Loss: 0.593573, Acc: 0.809417\n",
      "Test Loss: 0.612140, Acc: 0.796326\n",
      "[15/100] Loss: 0.575939, Acc: 0.821042\n",
      "[15/100] Loss: 0.583105, Acc: 0.816979\n",
      "[15/100] Loss: 0.589824, Acc: 0.812604\n",
      "[15/100] Loss: 0.590582, Acc: 0.811016\n",
      "[15/100] Loss: 0.589151, Acc: 0.810208\n",
      "[15/100] Loss: 0.586487, Acc: 0.811684\n",
      "Finish 15 epochs, Loss: 0.586114, Acc: 0.811650\n",
      "Test Loss: 0.605287, Acc: 0.798223\n",
      "[16/100] Loss: 0.579306, Acc: 0.811458\n",
      "[16/100] Loss: 0.577522, Acc: 0.814479\n",
      "[16/100] Loss: 0.574666, Acc: 0.815347\n",
      "[16/100] Loss: 0.579173, Acc: 0.813359\n",
      "[16/100] Loss: 0.579474, Acc: 0.813042\n",
      "[16/100] Loss: 0.579698, Acc: 0.813455\n",
      "Finish 16 epochs, Loss: 0.579214, Acc: 0.813617\n",
      "Test Loss: 0.599470, Acc: 0.801418\n",
      "[17/100] Loss: 0.576030, Acc: 0.817604\n",
      "[17/100] Loss: 0.576367, Acc: 0.816146\n",
      "[17/100] Loss: 0.576230, Acc: 0.815972\n",
      "[17/100] Loss: 0.576130, Acc: 0.815052\n",
      "[17/100] Loss: 0.575514, Acc: 0.814687\n",
      "[17/100] Loss: 0.573975, Acc: 0.814861\n",
      "Finish 17 epochs, Loss: 0.573074, Acc: 0.815067\n",
      "Test Loss: 0.593352, Acc: 0.803614\n",
      "[18/100] Loss: 0.570506, Acc: 0.816771\n",
      "[18/100] Loss: 0.572815, Acc: 0.815365\n",
      "[18/100] Loss: 0.568564, Acc: 0.815069\n",
      "[18/100] Loss: 0.570442, Acc: 0.815234\n",
      "[18/100] Loss: 0.568408, Acc: 0.816208\n",
      "[18/100] Loss: 0.567868, Acc: 0.816493\n",
      "Finish 18 epochs, Loss: 0.567349, Acc: 0.816967\n",
      "Test Loss: 0.588292, Acc: 0.805312\n",
      "[19/100] Loss: 0.566073, Acc: 0.818646\n",
      "[19/100] Loss: 0.570777, Acc: 0.815208\n",
      "[19/100] Loss: 0.562815, Acc: 0.820104\n",
      "[19/100] Loss: 0.559867, Acc: 0.820417\n",
      "[19/100] Loss: 0.560533, Acc: 0.819000\n",
      "[19/100] Loss: 0.561638, Acc: 0.818438\n",
      "Finish 19 epochs, Loss: 0.562066, Acc: 0.818167\n",
      "Test Loss: 0.583550, Acc: 0.807708\n",
      "[20/100] Loss: 0.578396, Acc: 0.811146\n",
      "[20/100] Loss: 0.565958, Acc: 0.815469\n",
      "[20/100] Loss: 0.564563, Acc: 0.814583\n",
      "[20/100] Loss: 0.561155, Acc: 0.816771\n",
      "[20/100] Loss: 0.560513, Acc: 0.817271\n",
      "[20/100] Loss: 0.557396, Acc: 0.819028\n",
      "Finish 20 epochs, Loss: 0.557250, Acc: 0.819467\n",
      "Test Loss: 0.579180, Acc: 0.808606\n",
      "[21/100] Loss: 0.545809, Acc: 0.822396\n",
      "[21/100] Loss: 0.551870, Acc: 0.820521\n",
      "[21/100] Loss: 0.553714, Acc: 0.819653\n",
      "[21/100] Loss: 0.556344, Acc: 0.819323\n",
      "[21/100] Loss: 0.552648, Acc: 0.820708\n",
      "[21/100] Loss: 0.552437, Acc: 0.820764\n",
      "Finish 21 epochs, Loss: 0.552700, Acc: 0.820667\n",
      "Test Loss: 0.575526, Acc: 0.809804\n",
      "[22/100] Loss: 0.559411, Acc: 0.820729\n",
      "[22/100] Loss: 0.554769, Acc: 0.821510\n",
      "[22/100] Loss: 0.551846, Acc: 0.821250\n",
      "[22/100] Loss: 0.548662, Acc: 0.821901\n",
      "[22/100] Loss: 0.548755, Acc: 0.821604\n",
      "[22/100] Loss: 0.547523, Acc: 0.822344\n",
      "Finish 22 epochs, Loss: 0.548607, Acc: 0.821817\n",
      "Test Loss: 0.571811, Acc: 0.810603\n",
      "[23/100] Loss: 0.539726, Acc: 0.823333\n",
      "[23/100] Loss: 0.540752, Acc: 0.823594\n",
      "[23/100] Loss: 0.540282, Acc: 0.823750\n",
      "[23/100] Loss: 0.542030, Acc: 0.823464\n",
      "[23/100] Loss: 0.544079, Acc: 0.822729\n",
      "[23/100] Loss: 0.544399, Acc: 0.823073\n",
      "Finish 23 epochs, Loss: 0.544607, Acc: 0.822950\n",
      "Test Loss: 0.567694, Acc: 0.811202\n",
      "[24/100] Loss: 0.541920, Acc: 0.825104\n",
      "[24/100] Loss: 0.545292, Acc: 0.821979\n",
      "[24/100] Loss: 0.545142, Acc: 0.822674\n",
      "[24/100] Loss: 0.541431, Acc: 0.823958\n",
      "[24/100] Loss: 0.542738, Acc: 0.823625\n",
      "[24/100] Loss: 0.541289, Acc: 0.823663\n",
      "Finish 24 epochs, Loss: 0.540881, Acc: 0.823583\n",
      "Test Loss: 0.564155, Acc: 0.812800\n",
      "[25/100] Loss: 0.538839, Acc: 0.821250\n",
      "[25/100] Loss: 0.542520, Acc: 0.820417\n",
      "[25/100] Loss: 0.539973, Acc: 0.822639\n",
      "[25/100] Loss: 0.540020, Acc: 0.823073\n",
      "[25/100] Loss: 0.540132, Acc: 0.824542\n",
      "[25/100] Loss: 0.538290, Acc: 0.825139\n",
      "Finish 25 epochs, Loss: 0.537420, Acc: 0.825067\n",
      "Test Loss: 0.560929, Acc: 0.813698\n",
      "[26/100] Loss: 0.531778, Acc: 0.827500\n",
      "[26/100] Loss: 0.527418, Acc: 0.827812\n",
      "[26/100] Loss: 0.528892, Acc: 0.827569\n",
      "[26/100] Loss: 0.529276, Acc: 0.826771\n",
      "[26/100] Loss: 0.533459, Acc: 0.825417\n",
      "[26/100] Loss: 0.532936, Acc: 0.826007\n",
      "Finish 26 epochs, Loss: 0.534176, Acc: 0.825583\n",
      "Test Loss: 0.557892, Acc: 0.814197\n",
      "[27/100] Loss: 0.520414, Acc: 0.831771\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[27/100] Loss: 0.522101, Acc: 0.830260\n",
      "[27/100] Loss: 0.531805, Acc: 0.826493\n",
      "[27/100] Loss: 0.534425, Acc: 0.825234\n",
      "[27/100] Loss: 0.534228, Acc: 0.825042\n",
      "[27/100] Loss: 0.531218, Acc: 0.826024\n",
      "Finish 27 epochs, Loss: 0.530987, Acc: 0.826300\n",
      "Test Loss: 0.555455, Acc: 0.813698\n",
      "[28/100] Loss: 0.520930, Acc: 0.830729\n",
      "[28/100] Loss: 0.528670, Acc: 0.826667\n",
      "[28/100] Loss: 0.529773, Acc: 0.826806\n",
      "[28/100] Loss: 0.531109, Acc: 0.825990\n",
      "[28/100] Loss: 0.531545, Acc: 0.826042\n",
      "[28/100] Loss: 0.528942, Acc: 0.827031\n",
      "Finish 28 epochs, Loss: 0.528042, Acc: 0.827450\n",
      "Test Loss: 0.552621, Acc: 0.815994\n",
      "[29/100] Loss: 0.527990, Acc: 0.830625\n",
      "[29/100] Loss: 0.525937, Acc: 0.828854\n",
      "[29/100] Loss: 0.527553, Acc: 0.827569\n",
      "[29/100] Loss: 0.523458, Acc: 0.828750\n",
      "[29/100] Loss: 0.521943, Acc: 0.829146\n",
      "[29/100] Loss: 0.525063, Acc: 0.828351\n",
      "Finish 29 epochs, Loss: 0.525295, Acc: 0.828400\n",
      "Test Loss: 0.549932, Acc: 0.816094\n",
      "[30/100] Loss: 0.515621, Acc: 0.832500\n",
      "[30/100] Loss: 0.516499, Acc: 0.833854\n",
      "[30/100] Loss: 0.520942, Acc: 0.831771\n",
      "[30/100] Loss: 0.520704, Acc: 0.829557\n",
      "[30/100] Loss: 0.520014, Acc: 0.829208\n",
      "[30/100] Loss: 0.521667, Acc: 0.828785\n",
      "Finish 30 epochs, Loss: 0.522630, Acc: 0.828283\n",
      "Test Loss: 0.547692, Acc: 0.816294\n",
      "[31/100] Loss: 0.510310, Acc: 0.830208\n",
      "[31/100] Loss: 0.517118, Acc: 0.827500\n",
      "[31/100] Loss: 0.519529, Acc: 0.828576\n",
      "[31/100] Loss: 0.517548, Acc: 0.829167\n",
      "[31/100] Loss: 0.516888, Acc: 0.830021\n",
      "[31/100] Loss: 0.519114, Acc: 0.829201\n",
      "Finish 31 epochs, Loss: 0.520060, Acc: 0.829133\n",
      "Test Loss: 0.545567, Acc: 0.817292\n",
      "[32/100] Loss: 0.533024, Acc: 0.824792\n",
      "[32/100] Loss: 0.524858, Acc: 0.828021\n",
      "[32/100] Loss: 0.520777, Acc: 0.829688\n",
      "[32/100] Loss: 0.518892, Acc: 0.829766\n",
      "[32/100] Loss: 0.519598, Acc: 0.829500\n",
      "[32/100] Loss: 0.518387, Acc: 0.829965\n",
      "Finish 32 epochs, Loss: 0.517642, Acc: 0.829983\n",
      "Test Loss: 0.543233, Acc: 0.817192\n",
      "[33/100] Loss: 0.508821, Acc: 0.831458\n",
      "[33/100] Loss: 0.510570, Acc: 0.832292\n",
      "[33/100] Loss: 0.516475, Acc: 0.829271\n",
      "[33/100] Loss: 0.517494, Acc: 0.829479\n",
      "[33/100] Loss: 0.516224, Acc: 0.830021\n",
      "[33/100] Loss: 0.514998, Acc: 0.830521\n",
      "Finish 33 epochs, Loss: 0.515344, Acc: 0.830367\n",
      "Test Loss: 0.541053, Acc: 0.818391\n",
      "[34/100] Loss: 0.508509, Acc: 0.830938\n",
      "[34/100] Loss: 0.509603, Acc: 0.832500\n",
      "[34/100] Loss: 0.511587, Acc: 0.830799\n",
      "[34/100] Loss: 0.513370, Acc: 0.829974\n",
      "[34/100] Loss: 0.512019, Acc: 0.830938\n",
      "[34/100] Loss: 0.512369, Acc: 0.830799\n",
      "Finish 34 epochs, Loss: 0.513142, Acc: 0.830250\n",
      "Test Loss: 0.539312, Acc: 0.818391\n",
      "[35/100] Loss: 0.509495, Acc: 0.832188\n",
      "[35/100] Loss: 0.507737, Acc: 0.832344\n",
      "[35/100] Loss: 0.507627, Acc: 0.833333\n",
      "[35/100] Loss: 0.508258, Acc: 0.832891\n",
      "[35/100] Loss: 0.511242, Acc: 0.831771\n",
      "[35/100] Loss: 0.510738, Acc: 0.831736\n",
      "Finish 35 epochs, Loss: 0.510962, Acc: 0.831317\n",
      "Test Loss: 0.537989, Acc: 0.819289\n",
      "[36/100] Loss: 0.504811, Acc: 0.837396\n",
      "[36/100] Loss: 0.507694, Acc: 0.836198\n",
      "[36/100] Loss: 0.509293, Acc: 0.834514\n",
      "[36/100] Loss: 0.507826, Acc: 0.832917\n",
      "[36/100] Loss: 0.509788, Acc: 0.832167\n",
      "[36/100] Loss: 0.509338, Acc: 0.832205\n",
      "Finish 36 epochs, Loss: 0.508950, Acc: 0.832433\n",
      "Test Loss: 0.535481, Acc: 0.819389\n",
      "[37/100] Loss: 0.511838, Acc: 0.830833\n",
      "[37/100] Loss: 0.503313, Acc: 0.834010\n",
      "[37/100] Loss: 0.505658, Acc: 0.832500\n",
      "[37/100] Loss: 0.504949, Acc: 0.832995\n",
      "[37/100] Loss: 0.507339, Acc: 0.831979\n",
      "[37/100] Loss: 0.506504, Acc: 0.832587\n",
      "Finish 37 epochs, Loss: 0.506994, Acc: 0.832300\n",
      "Test Loss: 0.533462, Acc: 0.820387\n",
      "[38/100] Loss: 0.496869, Acc: 0.831563\n",
      "[38/100] Loss: 0.495549, Acc: 0.835938\n",
      "[38/100] Loss: 0.498925, Acc: 0.835000\n",
      "[38/100] Loss: 0.500154, Acc: 0.835260\n",
      "[38/100] Loss: 0.502708, Acc: 0.834125\n",
      "[38/100] Loss: 0.504160, Acc: 0.833681\n",
      "Finish 38 epochs, Loss: 0.505124, Acc: 0.833200\n",
      "Test Loss: 0.532360, Acc: 0.821086\n",
      "[39/100] Loss: 0.498618, Acc: 0.832604\n",
      "[39/100] Loss: 0.504598, Acc: 0.830156\n",
      "[39/100] Loss: 0.500694, Acc: 0.833160\n",
      "[39/100] Loss: 0.503251, Acc: 0.833594\n",
      "[39/100] Loss: 0.504920, Acc: 0.833792\n",
      "[39/100] Loss: 0.502430, Acc: 0.834184\n",
      "Finish 39 epochs, Loss: 0.503363, Acc: 0.833700\n",
      "Test Loss: 0.530198, Acc: 0.821585\n",
      "[40/100] Loss: 0.505760, Acc: 0.831667\n",
      "[40/100] Loss: 0.502239, Acc: 0.832188\n",
      "[40/100] Loss: 0.501800, Acc: 0.833264\n",
      "[40/100] Loss: 0.501318, Acc: 0.834505\n",
      "[40/100] Loss: 0.503212, Acc: 0.833792\n",
      "[40/100] Loss: 0.501174, Acc: 0.834844\n",
      "Finish 40 epochs, Loss: 0.501551, Acc: 0.834383\n",
      "Test Loss: 0.528704, Acc: 0.821585\n",
      "[41/100] Loss: 0.501234, Acc: 0.830000\n",
      "[41/100] Loss: 0.496085, Acc: 0.834010\n",
      "[41/100] Loss: 0.498391, Acc: 0.835104\n",
      "[41/100] Loss: 0.498292, Acc: 0.835182\n",
      "[41/100] Loss: 0.499211, Acc: 0.834812\n",
      "[41/100] Loss: 0.499239, Acc: 0.834497\n",
      "Finish 41 epochs, Loss: 0.499886, Acc: 0.834250\n",
      "Test Loss: 0.527415, Acc: 0.822684\n",
      "[42/100] Loss: 0.505824, Acc: 0.832813\n",
      "[42/100] Loss: 0.501714, Acc: 0.833281\n",
      "[42/100] Loss: 0.500417, Acc: 0.834618\n",
      "[42/100] Loss: 0.499933, Acc: 0.834896\n",
      "[42/100] Loss: 0.497920, Acc: 0.835896\n",
      "[42/100] Loss: 0.498176, Acc: 0.835417\n",
      "Finish 42 epochs, Loss: 0.498200, Acc: 0.835150\n",
      "Test Loss: 0.526352, Acc: 0.822784\n",
      "[43/100] Loss: 0.483611, Acc: 0.840104\n",
      "[43/100] Loss: 0.494887, Acc: 0.837188\n",
      "[43/100] Loss: 0.494338, Acc: 0.836944\n",
      "[43/100] Loss: 0.495564, Acc: 0.836224\n",
      "[43/100] Loss: 0.495829, Acc: 0.836271\n",
      "[43/100] Loss: 0.497231, Acc: 0.835521\n",
      "Finish 43 epochs, Loss: 0.496700, Acc: 0.835933\n",
      "Test Loss: 0.524264, Acc: 0.823982\n",
      "[44/100] Loss: 0.509487, Acc: 0.829167\n",
      "[44/100] Loss: 0.500449, Acc: 0.835260\n",
      "[44/100] Loss: 0.500314, Acc: 0.834688\n",
      "[44/100] Loss: 0.500076, Acc: 0.834531\n",
      "[44/100] Loss: 0.499856, Acc: 0.834833\n",
      "[44/100] Loss: 0.495525, Acc: 0.835816\n",
      "Finish 44 epochs, Loss: 0.495106, Acc: 0.835933\n",
      "Test Loss: 0.523144, Acc: 0.824181\n",
      "[45/100] Loss: 0.492563, Acc: 0.838542\n",
      "[45/100] Loss: 0.489930, Acc: 0.837865\n",
      "[45/100] Loss: 0.496596, Acc: 0.835451\n",
      "[45/100] Loss: 0.495445, Acc: 0.836354\n",
      "[45/100] Loss: 0.495208, Acc: 0.836583\n",
      "[45/100] Loss: 0.494703, Acc: 0.836059\n",
      "Finish 45 epochs, Loss: 0.493682, Acc: 0.836617\n",
      "Test Loss: 0.521850, Acc: 0.824281\n",
      "[46/100] Loss: 0.497778, Acc: 0.831563\n",
      "[46/100] Loss: 0.496087, Acc: 0.833594\n",
      "[46/100] Loss: 0.489798, Acc: 0.837118\n",
      "[46/100] Loss: 0.492250, Acc: 0.837214\n",
      "[46/100] Loss: 0.491339, Acc: 0.837292\n",
      "[46/100] Loss: 0.492003, Acc: 0.836962\n",
      "Finish 46 epochs, Loss: 0.492273, Acc: 0.836667\n",
      "Test Loss: 0.520282, Acc: 0.825080\n",
      "[47/100] Loss: 0.482865, Acc: 0.842083\n",
      "[47/100] Loss: 0.482626, Acc: 0.840156\n",
      "[47/100] Loss: 0.487432, Acc: 0.837604\n",
      "[47/100] Loss: 0.488192, Acc: 0.837500\n",
      "[47/100] Loss: 0.489562, Acc: 0.837646\n",
      "[47/100] Loss: 0.491277, Acc: 0.837257\n",
      "Finish 47 epochs, Loss: 0.490862, Acc: 0.837367\n",
      "Test Loss: 0.519074, Acc: 0.825679\n",
      "[48/100] Loss: 0.489593, Acc: 0.836354\n",
      "[48/100] Loss: 0.488540, Acc: 0.836458\n",
      "[48/100] Loss: 0.490600, Acc: 0.836667\n",
      "[48/100] Loss: 0.491354, Acc: 0.836458\n",
      "[48/100] Loss: 0.490714, Acc: 0.837250\n",
      "[48/100] Loss: 0.489392, Acc: 0.837986\n",
      "Finish 48 epochs, Loss: 0.489549, Acc: 0.837650\n",
      "Test Loss: 0.518248, Acc: 0.826178\n",
      "[49/100] Loss: 0.500338, Acc: 0.834479\n",
      "[49/100] Loss: 0.493509, Acc: 0.835885\n",
      "[49/100] Loss: 0.490454, Acc: 0.836736\n",
      "[49/100] Loss: 0.488173, Acc: 0.838281\n",
      "[49/100] Loss: 0.489061, Acc: 0.837812\n",
      "[49/100] Loss: 0.488408, Acc: 0.837865\n",
      "Finish 49 epochs, Loss: 0.488203, Acc: 0.837817\n",
      "Test Loss: 0.516741, Acc: 0.826777\n",
      "[50/100] Loss: 0.485611, Acc: 0.839688\n",
      "[50/100] Loss: 0.490046, Acc: 0.836458\n",
      "[50/100] Loss: 0.487680, Acc: 0.837083\n",
      "[50/100] Loss: 0.489061, Acc: 0.837240\n",
      "[50/100] Loss: 0.486799, Acc: 0.838521\n",
      "[50/100] Loss: 0.486377, Acc: 0.838507\n",
      "Finish 50 epochs, Loss: 0.486905, Acc: 0.838150\n",
      "Test Loss: 0.515673, Acc: 0.826278\n",
      "[51/100] Loss: 0.500547, Acc: 0.833438\n",
      "[51/100] Loss: 0.487540, Acc: 0.838333\n",
      "[51/100] Loss: 0.484146, Acc: 0.838507\n",
      "[51/100] Loss: 0.487733, Acc: 0.837474\n",
      "[51/100] Loss: 0.487753, Acc: 0.837667\n",
      "[51/100] Loss: 0.486141, Acc: 0.838368\n",
      "Finish 51 epochs, Loss: 0.485783, Acc: 0.838550\n",
      "Test Loss: 0.514569, Acc: 0.827177\n",
      "[52/100] Loss: 0.478276, Acc: 0.840417\n",
      "[52/100] Loss: 0.480469, Acc: 0.839063\n",
      "[52/100] Loss: 0.484899, Acc: 0.837257\n",
      "[52/100] Loss: 0.484700, Acc: 0.838333\n",
      "[52/100] Loss: 0.485179, Acc: 0.838271\n",
      "[52/100] Loss: 0.484264, Acc: 0.838472\n",
      "Finish 52 epochs, Loss: 0.484560, Acc: 0.838367\n",
      "Test Loss: 0.513667, Acc: 0.827376\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[53/100] Loss: 0.483478, Acc: 0.839479\n",
      "[53/100] Loss: 0.477175, Acc: 0.840833\n",
      "[53/100] Loss: 0.482066, Acc: 0.839271\n",
      "[53/100] Loss: 0.479484, Acc: 0.840912\n",
      "[53/100] Loss: 0.479818, Acc: 0.840937\n",
      "[53/100] Loss: 0.482524, Acc: 0.839948\n",
      "Finish 53 epochs, Loss: 0.483365, Acc: 0.839467\n",
      "Test Loss: 0.512550, Acc: 0.827576\n",
      "[54/100] Loss: 0.483907, Acc: 0.836458\n",
      "[54/100] Loss: 0.483653, Acc: 0.838281\n",
      "[54/100] Loss: 0.485014, Acc: 0.837813\n",
      "[54/100] Loss: 0.483167, Acc: 0.838437\n",
      "[54/100] Loss: 0.483566, Acc: 0.838604\n",
      "[54/100] Loss: 0.483060, Acc: 0.839219\n",
      "Finish 54 epochs, Loss: 0.482185, Acc: 0.839467\n",
      "Test Loss: 0.511605, Acc: 0.827177\n",
      "[55/100] Loss: 0.490533, Acc: 0.835625\n",
      "[55/100] Loss: 0.481837, Acc: 0.838750\n",
      "[55/100] Loss: 0.481759, Acc: 0.838229\n",
      "[55/100] Loss: 0.480920, Acc: 0.838672\n",
      "[55/100] Loss: 0.481094, Acc: 0.839854\n",
      "[55/100] Loss: 0.481499, Acc: 0.839705\n",
      "Finish 55 epochs, Loss: 0.481209, Acc: 0.839950\n",
      "Test Loss: 0.510561, Acc: 0.827875\n",
      "[56/100] Loss: 0.471742, Acc: 0.844375\n",
      "[56/100] Loss: 0.474352, Acc: 0.842917\n",
      "[56/100] Loss: 0.476987, Acc: 0.841250\n",
      "[56/100] Loss: 0.481291, Acc: 0.841042\n",
      "[56/100] Loss: 0.482456, Acc: 0.839813\n",
      "[56/100] Loss: 0.479868, Acc: 0.840243\n",
      "Finish 56 epochs, Loss: 0.480081, Acc: 0.840333\n",
      "Test Loss: 0.509436, Acc: 0.828375\n",
      "[57/100] Loss: 0.475252, Acc: 0.841250\n",
      "[57/100] Loss: 0.474073, Acc: 0.842708\n",
      "[57/100] Loss: 0.473311, Acc: 0.843299\n",
      "[57/100] Loss: 0.476132, Acc: 0.842682\n",
      "[57/100] Loss: 0.476558, Acc: 0.842229\n",
      "[57/100] Loss: 0.479740, Acc: 0.840243\n",
      "Finish 57 epochs, Loss: 0.479061, Acc: 0.840400\n",
      "Test Loss: 0.508617, Acc: 0.828574\n",
      "[58/100] Loss: 0.477308, Acc: 0.841875\n",
      "[58/100] Loss: 0.483343, Acc: 0.839948\n",
      "[58/100] Loss: 0.478279, Acc: 0.841111\n",
      "[58/100] Loss: 0.475489, Acc: 0.842422\n",
      "[58/100] Loss: 0.476372, Acc: 0.841958\n",
      "[58/100] Loss: 0.477235, Acc: 0.840833\n",
      "Finish 58 epochs, Loss: 0.478039, Acc: 0.840750\n",
      "Test Loss: 0.507820, Acc: 0.829073\n",
      "[59/100] Loss: 0.485002, Acc: 0.838229\n",
      "[59/100] Loss: 0.482400, Acc: 0.840000\n",
      "[59/100] Loss: 0.482921, Acc: 0.838194\n",
      "[59/100] Loss: 0.479733, Acc: 0.839167\n",
      "[59/100] Loss: 0.477359, Acc: 0.840958\n",
      "[59/100] Loss: 0.477260, Acc: 0.840885\n",
      "Finish 59 epochs, Loss: 0.477041, Acc: 0.840667\n",
      "Test Loss: 0.507250, Acc: 0.827875\n",
      "[60/100] Loss: 0.485919, Acc: 0.834792\n",
      "[60/100] Loss: 0.476818, Acc: 0.841979\n",
      "[60/100] Loss: 0.476888, Acc: 0.841597\n",
      "[60/100] Loss: 0.477136, Acc: 0.840755\n",
      "[60/100] Loss: 0.475932, Acc: 0.841271\n",
      "[60/100] Loss: 0.476694, Acc: 0.841059\n",
      "Finish 60 epochs, Loss: 0.476103, Acc: 0.841333\n",
      "Test Loss: 0.506431, Acc: 0.828874\n",
      "[61/100] Loss: 0.481569, Acc: 0.838854\n",
      "[61/100] Loss: 0.476737, Acc: 0.840000\n",
      "[61/100] Loss: 0.479760, Acc: 0.838368\n",
      "[61/100] Loss: 0.475543, Acc: 0.839740\n",
      "[61/100] Loss: 0.474559, Acc: 0.841021\n",
      "[61/100] Loss: 0.475309, Acc: 0.841111\n",
      "Finish 61 epochs, Loss: 0.475066, Acc: 0.841433\n",
      "Test Loss: 0.505093, Acc: 0.829273\n",
      "[62/100] Loss: 0.458558, Acc: 0.848229\n",
      "[62/100] Loss: 0.465542, Acc: 0.844115\n",
      "[62/100] Loss: 0.467309, Acc: 0.845486\n",
      "[62/100] Loss: 0.470423, Acc: 0.843802\n",
      "[62/100] Loss: 0.473117, Acc: 0.842479\n",
      "[62/100] Loss: 0.473993, Acc: 0.842066\n",
      "Finish 62 epochs, Loss: 0.474189, Acc: 0.841883\n",
      "Test Loss: 0.504740, Acc: 0.827875\n",
      "[63/100] Loss: 0.482015, Acc: 0.838333\n",
      "[63/100] Loss: 0.484868, Acc: 0.837500\n",
      "[63/100] Loss: 0.481002, Acc: 0.839757\n",
      "[63/100] Loss: 0.475493, Acc: 0.841771\n",
      "[63/100] Loss: 0.473238, Acc: 0.842313\n",
      "[63/100] Loss: 0.473387, Acc: 0.842118\n",
      "Finish 63 epochs, Loss: 0.473278, Acc: 0.842000\n",
      "Test Loss: 0.503662, Acc: 0.828375\n",
      "[64/100] Loss: 0.483648, Acc: 0.838333\n",
      "[64/100] Loss: 0.478916, Acc: 0.840677\n",
      "[64/100] Loss: 0.472174, Acc: 0.841076\n",
      "[64/100] Loss: 0.471337, Acc: 0.842240\n",
      "[64/100] Loss: 0.473627, Acc: 0.841646\n",
      "[64/100] Loss: 0.471418, Acc: 0.842847\n",
      "Finish 64 epochs, Loss: 0.472374, Acc: 0.842700\n",
      "Test Loss: 0.503173, Acc: 0.829373\n",
      "[65/100] Loss: 0.476810, Acc: 0.844688\n",
      "[65/100] Loss: 0.477152, Acc: 0.842240\n",
      "[65/100] Loss: 0.473201, Acc: 0.841910\n",
      "[65/100] Loss: 0.472456, Acc: 0.842161\n",
      "[65/100] Loss: 0.472853, Acc: 0.841563\n",
      "[65/100] Loss: 0.471320, Acc: 0.842917\n",
      "Finish 65 epochs, Loss: 0.471564, Acc: 0.842850\n",
      "Test Loss: 0.502090, Acc: 0.830871\n",
      "[66/100] Loss: 0.463267, Acc: 0.850729\n",
      "[66/100] Loss: 0.467539, Acc: 0.846927\n",
      "[66/100] Loss: 0.467977, Acc: 0.845590\n",
      "[66/100] Loss: 0.470337, Acc: 0.843672\n",
      "[66/100] Loss: 0.470534, Acc: 0.842917\n",
      "[66/100] Loss: 0.471568, Acc: 0.842847\n",
      "Finish 66 epochs, Loss: 0.470747, Acc: 0.843350\n",
      "Test Loss: 0.501151, Acc: 0.830272\n",
      "[67/100] Loss: 0.466697, Acc: 0.840729\n",
      "[67/100] Loss: 0.463077, Acc: 0.844479\n",
      "[67/100] Loss: 0.464021, Acc: 0.844896\n",
      "[67/100] Loss: 0.467882, Acc: 0.843021\n",
      "[67/100] Loss: 0.467471, Acc: 0.843375\n",
      "[67/100] Loss: 0.469647, Acc: 0.843264\n",
      "Finish 67 epochs, Loss: 0.469896, Acc: 0.843383\n",
      "Test Loss: 0.500493, Acc: 0.829972\n",
      "[68/100] Loss: 0.476954, Acc: 0.840625\n",
      "[68/100] Loss: 0.467262, Acc: 0.846146\n",
      "[68/100] Loss: 0.464414, Acc: 0.846076\n",
      "[68/100] Loss: 0.462709, Acc: 0.846667\n",
      "[68/100] Loss: 0.465866, Acc: 0.845187\n",
      "[68/100] Loss: 0.467594, Acc: 0.844184\n",
      "Finish 68 epochs, Loss: 0.469008, Acc: 0.843700\n",
      "Test Loss: 0.499968, Acc: 0.830471\n",
      "[69/100] Loss: 0.480649, Acc: 0.835625\n",
      "[69/100] Loss: 0.470616, Acc: 0.840781\n",
      "[69/100] Loss: 0.466874, Acc: 0.843056\n",
      "[69/100] Loss: 0.467593, Acc: 0.844401\n",
      "[69/100] Loss: 0.468008, Acc: 0.844083\n",
      "[69/100] Loss: 0.467665, Acc: 0.844253\n",
      "Finish 69 epochs, Loss: 0.468348, Acc: 0.843750\n",
      "Test Loss: 0.499088, Acc: 0.830471\n",
      "[70/100] Loss: 0.465247, Acc: 0.847604\n",
      "[70/100] Loss: 0.458026, Acc: 0.849010\n",
      "[70/100] Loss: 0.461538, Acc: 0.846910\n",
      "[70/100] Loss: 0.463179, Acc: 0.845495\n",
      "[70/100] Loss: 0.466465, Acc: 0.844875\n",
      "[70/100] Loss: 0.465851, Acc: 0.844913\n",
      "Finish 70 epochs, Loss: 0.467497, Acc: 0.844300\n",
      "Test Loss: 0.498388, Acc: 0.830671\n",
      "[71/100] Loss: 0.468625, Acc: 0.841042\n",
      "[71/100] Loss: 0.466376, Acc: 0.843437\n",
      "[71/100] Loss: 0.466663, Acc: 0.844583\n",
      "[71/100] Loss: 0.467982, Acc: 0.843854\n",
      "[71/100] Loss: 0.467351, Acc: 0.843833\n",
      "[71/100] Loss: 0.467073, Acc: 0.843646\n",
      "Finish 71 epochs, Loss: 0.466683, Acc: 0.844050\n",
      "Test Loss: 0.497811, Acc: 0.830771\n",
      "[72/100] Loss: 0.464128, Acc: 0.842292\n",
      "[72/100] Loss: 0.468932, Acc: 0.843490\n",
      "[72/100] Loss: 0.467063, Acc: 0.844861\n",
      "[72/100] Loss: 0.466112, Acc: 0.844635\n",
      "[72/100] Loss: 0.465961, Acc: 0.843792\n",
      "[72/100] Loss: 0.466777, Acc: 0.843767\n",
      "Finish 72 epochs, Loss: 0.465960, Acc: 0.844250\n",
      "Test Loss: 0.497264, Acc: 0.830371\n",
      "[73/100] Loss: 0.469683, Acc: 0.841667\n",
      "[73/100] Loss: 0.467832, Acc: 0.842917\n",
      "[73/100] Loss: 0.465531, Acc: 0.843299\n",
      "[73/100] Loss: 0.464700, Acc: 0.843724\n",
      "[73/100] Loss: 0.466457, Acc: 0.843562\n",
      "[73/100] Loss: 0.463630, Acc: 0.845243\n",
      "Finish 73 epochs, Loss: 0.465284, Acc: 0.844717\n",
      "Test Loss: 0.496530, Acc: 0.831470\n",
      "[74/100] Loss: 0.461092, Acc: 0.848750\n",
      "[74/100] Loss: 0.460452, Acc: 0.847344\n",
      "[74/100] Loss: 0.464189, Acc: 0.845660\n",
      "[74/100] Loss: 0.464874, Acc: 0.845260\n",
      "[74/100] Loss: 0.464912, Acc: 0.845333\n",
      "[74/100] Loss: 0.463922, Acc: 0.845191\n",
      "Finish 74 epochs, Loss: 0.464463, Acc: 0.844933\n",
      "Test Loss: 0.496014, Acc: 0.831370\n",
      "[75/100] Loss: 0.452676, Acc: 0.847083\n",
      "[75/100] Loss: 0.454520, Acc: 0.847969\n",
      "[75/100] Loss: 0.466291, Acc: 0.844688\n",
      "[75/100] Loss: 0.465375, Acc: 0.843542\n",
      "[75/100] Loss: 0.463431, Acc: 0.844521\n",
      "[75/100] Loss: 0.464386, Acc: 0.844618\n",
      "Finish 75 epochs, Loss: 0.463792, Acc: 0.844933\n",
      "Test Loss: 0.495258, Acc: 0.830771\n",
      "[76/100] Loss: 0.471055, Acc: 0.842708\n",
      "[76/100] Loss: 0.474302, Acc: 0.843594\n",
      "[76/100] Loss: 0.468735, Acc: 0.844063\n",
      "[76/100] Loss: 0.467438, Acc: 0.844375\n",
      "[76/100] Loss: 0.465325, Acc: 0.844542\n",
      "[76/100] Loss: 0.462750, Acc: 0.845313\n",
      "Finish 76 epochs, Loss: 0.463077, Acc: 0.845033\n",
      "Test Loss: 0.495022, Acc: 0.830871\n",
      "[77/100] Loss: 0.471200, Acc: 0.843750\n",
      "[77/100] Loss: 0.463637, Acc: 0.846042\n",
      "[77/100] Loss: 0.463643, Acc: 0.846319\n",
      "[77/100] Loss: 0.461839, Acc: 0.847865\n",
      "[77/100] Loss: 0.463555, Acc: 0.846125\n",
      "[77/100] Loss: 0.463911, Acc: 0.845486\n",
      "Finish 77 epochs, Loss: 0.462524, Acc: 0.845817\n",
      "Test Loss: 0.493951, Acc: 0.832268\n",
      "[78/100] Loss: 0.466624, Acc: 0.846042\n",
      "[78/100] Loss: 0.468468, Acc: 0.845365\n",
      "[78/100] Loss: 0.466415, Acc: 0.845313\n",
      "[78/100] Loss: 0.463784, Acc: 0.844844\n",
      "[78/100] Loss: 0.461411, Acc: 0.845208\n",
      "[78/100] Loss: 0.461792, Acc: 0.845260\n",
      "Finish 78 epochs, Loss: 0.461787, Acc: 0.845317\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.493444, Acc: 0.831769\n",
      "[79/100] Loss: 0.457140, Acc: 0.849792\n",
      "[79/100] Loss: 0.461303, Acc: 0.846667\n",
      "[79/100] Loss: 0.463241, Acc: 0.844757\n",
      "[79/100] Loss: 0.462015, Acc: 0.844974\n",
      "[79/100] Loss: 0.462755, Acc: 0.844729\n",
      "[79/100] Loss: 0.461504, Acc: 0.846059\n",
      "Finish 79 epochs, Loss: 0.461160, Acc: 0.846367\n",
      "Test Loss: 0.492807, Acc: 0.831270\n",
      "[80/100] Loss: 0.450505, Acc: 0.843646\n",
      "[80/100] Loss: 0.461581, Acc: 0.842604\n",
      "[80/100] Loss: 0.460216, Acc: 0.845694\n",
      "[80/100] Loss: 0.461253, Acc: 0.845573\n",
      "[80/100] Loss: 0.463024, Acc: 0.845104\n",
      "[80/100] Loss: 0.461042, Acc: 0.846024\n",
      "Finish 80 epochs, Loss: 0.460479, Acc: 0.846283\n",
      "Test Loss: 0.492632, Acc: 0.832568\n",
      "[81/100] Loss: 0.462285, Acc: 0.846354\n",
      "[81/100] Loss: 0.451453, Acc: 0.849115\n",
      "[81/100] Loss: 0.451432, Acc: 0.849201\n",
      "[81/100] Loss: 0.454844, Acc: 0.847839\n",
      "[81/100] Loss: 0.458211, Acc: 0.846792\n",
      "[81/100] Loss: 0.460077, Acc: 0.846059\n",
      "Finish 81 epochs, Loss: 0.459899, Acc: 0.846133\n",
      "Test Loss: 0.491841, Acc: 0.831070\n",
      "[82/100] Loss: 0.449170, Acc: 0.847396\n",
      "[82/100] Loss: 0.460983, Acc: 0.844792\n",
      "[82/100] Loss: 0.457824, Acc: 0.844757\n",
      "[82/100] Loss: 0.461683, Acc: 0.844323\n",
      "[82/100] Loss: 0.458403, Acc: 0.846042\n",
      "[82/100] Loss: 0.458790, Acc: 0.846059\n",
      "Finish 82 epochs, Loss: 0.459252, Acc: 0.846283\n",
      "Test Loss: 0.491267, Acc: 0.832069\n",
      "[83/100] Loss: 0.451116, Acc: 0.848750\n",
      "[83/100] Loss: 0.449935, Acc: 0.848073\n",
      "[83/100] Loss: 0.450244, Acc: 0.849792\n",
      "[83/100] Loss: 0.454160, Acc: 0.848594\n",
      "[83/100] Loss: 0.459826, Acc: 0.846479\n",
      "[83/100] Loss: 0.459864, Acc: 0.846181\n",
      "Finish 83 epochs, Loss: 0.458621, Acc: 0.846683\n",
      "Test Loss: 0.490811, Acc: 0.831370\n",
      "[84/100] Loss: 0.458707, Acc: 0.848750\n",
      "[84/100] Loss: 0.460266, Acc: 0.845052\n",
      "[84/100] Loss: 0.458113, Acc: 0.847014\n",
      "[84/100] Loss: 0.458880, Acc: 0.846016\n",
      "[84/100] Loss: 0.457726, Acc: 0.846646\n",
      "[84/100] Loss: 0.458169, Acc: 0.846701\n",
      "Finish 84 epochs, Loss: 0.458044, Acc: 0.846600\n",
      "Test Loss: 0.490141, Acc: 0.832568\n",
      "[85/100] Loss: 0.454977, Acc: 0.850938\n",
      "[85/100] Loss: 0.457542, Acc: 0.847917\n",
      "[85/100] Loss: 0.457908, Acc: 0.847569\n",
      "[85/100] Loss: 0.455508, Acc: 0.847370\n",
      "[85/100] Loss: 0.456699, Acc: 0.846917\n",
      "[85/100] Loss: 0.456659, Acc: 0.847118\n",
      "Finish 85 epochs, Loss: 0.457450, Acc: 0.846950\n",
      "Test Loss: 0.489651, Acc: 0.832568\n",
      "[86/100] Loss: 0.459808, Acc: 0.847708\n",
      "[86/100] Loss: 0.455409, Acc: 0.849948\n",
      "[86/100] Loss: 0.455571, Acc: 0.848507\n",
      "[86/100] Loss: 0.456303, Acc: 0.848646\n",
      "[86/100] Loss: 0.456639, Acc: 0.847583\n",
      "[86/100] Loss: 0.455641, Acc: 0.847500\n",
      "Finish 86 epochs, Loss: 0.456866, Acc: 0.847233\n",
      "Test Loss: 0.489191, Acc: 0.832268\n",
      "[87/100] Loss: 0.451087, Acc: 0.846875\n",
      "[87/100] Loss: 0.450966, Acc: 0.848333\n",
      "[87/100] Loss: 0.457956, Acc: 0.845833\n",
      "[87/100] Loss: 0.460373, Acc: 0.845339\n",
      "[87/100] Loss: 0.458623, Acc: 0.846604\n",
      "[87/100] Loss: 0.456577, Acc: 0.847135\n",
      "Finish 87 epochs, Loss: 0.456296, Acc: 0.847383\n",
      "Test Loss: 0.489041, Acc: 0.832368\n",
      "[88/100] Loss: 0.444459, Acc: 0.849479\n",
      "[88/100] Loss: 0.455428, Acc: 0.845313\n",
      "[88/100] Loss: 0.453386, Acc: 0.846563\n",
      "[88/100] Loss: 0.451780, Acc: 0.847813\n",
      "[88/100] Loss: 0.454350, Acc: 0.848125\n",
      "[88/100] Loss: 0.454918, Acc: 0.847361\n",
      "Finish 88 epochs, Loss: 0.455726, Acc: 0.847183\n",
      "Test Loss: 0.488427, Acc: 0.833267\n",
      "[89/100] Loss: 0.468670, Acc: 0.842917\n",
      "[89/100] Loss: 0.459297, Acc: 0.846406\n",
      "[89/100] Loss: 0.456511, Acc: 0.848056\n",
      "[89/100] Loss: 0.455516, Acc: 0.847891\n",
      "[89/100] Loss: 0.456762, Acc: 0.847438\n",
      "[89/100] Loss: 0.455523, Acc: 0.847448\n",
      "Finish 89 epochs, Loss: 0.455097, Acc: 0.847383\n",
      "Test Loss: 0.487735, Acc: 0.833566\n",
      "[90/100] Loss: 0.447924, Acc: 0.852500\n",
      "[90/100] Loss: 0.453036, Acc: 0.849896\n",
      "[90/100] Loss: 0.449590, Acc: 0.850069\n",
      "[90/100] Loss: 0.451288, Acc: 0.848646\n",
      "[90/100] Loss: 0.454066, Acc: 0.847396\n",
      "[90/100] Loss: 0.454530, Acc: 0.847396\n",
      "Finish 90 epochs, Loss: 0.454641, Acc: 0.847600\n",
      "Test Loss: 0.487338, Acc: 0.832668\n",
      "[91/100] Loss: 0.460210, Acc: 0.847292\n",
      "[91/100] Loss: 0.455932, Acc: 0.846979\n",
      "[91/100] Loss: 0.457780, Acc: 0.845833\n",
      "[91/100] Loss: 0.453877, Acc: 0.847813\n",
      "[91/100] Loss: 0.454282, Acc: 0.847771\n",
      "[91/100] Loss: 0.452857, Acc: 0.848247\n",
      "Finish 91 epochs, Loss: 0.454038, Acc: 0.847783\n",
      "Test Loss: 0.487146, Acc: 0.831370\n",
      "[92/100] Loss: 0.457959, Acc: 0.846354\n",
      "[92/100] Loss: 0.451205, Acc: 0.848958\n",
      "[92/100] Loss: 0.455466, Acc: 0.847292\n",
      "[92/100] Loss: 0.456595, Acc: 0.846927\n",
      "[92/100] Loss: 0.457680, Acc: 0.845521\n",
      "[92/100] Loss: 0.453365, Acc: 0.847813\n",
      "Finish 92 epochs, Loss: 0.453623, Acc: 0.847867\n",
      "Test Loss: 0.486270, Acc: 0.833866\n",
      "[93/100] Loss: 0.464957, Acc: 0.845417\n",
      "[93/100] Loss: 0.460860, Acc: 0.846510\n",
      "[93/100] Loss: 0.455671, Acc: 0.848611\n",
      "[93/100] Loss: 0.456263, Acc: 0.848073\n",
      "[93/100] Loss: 0.455774, Acc: 0.848167\n",
      "[93/100] Loss: 0.453554, Acc: 0.848194\n",
      "Finish 93 epochs, Loss: 0.453018, Acc: 0.848100\n",
      "Test Loss: 0.486018, Acc: 0.833466\n",
      "[94/100] Loss: 0.442010, Acc: 0.852083\n",
      "[94/100] Loss: 0.442489, Acc: 0.852708\n",
      "[94/100] Loss: 0.445272, Acc: 0.851597\n",
      "[94/100] Loss: 0.448527, Acc: 0.850052\n",
      "[94/100] Loss: 0.452097, Acc: 0.849146\n",
      "[94/100] Loss: 0.452557, Acc: 0.848941\n",
      "Finish 94 epochs, Loss: 0.452485, Acc: 0.848600\n",
      "Test Loss: 0.485582, Acc: 0.832468\n",
      "[95/100] Loss: 0.457296, Acc: 0.846250\n",
      "[95/100] Loss: 0.456367, Acc: 0.847240\n",
      "[95/100] Loss: 0.455207, Acc: 0.847847\n",
      "[95/100] Loss: 0.453089, Acc: 0.848229\n",
      "[95/100] Loss: 0.453560, Acc: 0.847729\n",
      "[95/100] Loss: 0.451056, Acc: 0.848819\n",
      "Finish 95 epochs, Loss: 0.452026, Acc: 0.848333\n",
      "Test Loss: 0.485251, Acc: 0.835064\n",
      "[96/100] Loss: 0.465026, Acc: 0.847188\n",
      "[96/100] Loss: 0.461582, Acc: 0.847240\n",
      "[96/100] Loss: 0.459228, Acc: 0.847813\n",
      "[96/100] Loss: 0.457231, Acc: 0.848151\n",
      "[96/100] Loss: 0.454955, Acc: 0.848188\n",
      "[96/100] Loss: 0.452853, Acc: 0.848004\n",
      "Finish 96 epochs, Loss: 0.451565, Acc: 0.848317\n",
      "Test Loss: 0.484610, Acc: 0.834565\n",
      "[97/100] Loss: 0.445156, Acc: 0.850313\n",
      "[97/100] Loss: 0.444747, Acc: 0.850417\n",
      "[97/100] Loss: 0.449241, Acc: 0.848264\n",
      "[97/100] Loss: 0.452491, Acc: 0.848047\n",
      "[97/100] Loss: 0.449926, Acc: 0.849333\n",
      "[97/100] Loss: 0.451155, Acc: 0.848559\n",
      "Finish 97 epochs, Loss: 0.451068, Acc: 0.848383\n",
      "Test Loss: 0.484379, Acc: 0.833067\n",
      "[98/100] Loss: 0.439713, Acc: 0.848750\n",
      "[98/100] Loss: 0.450044, Acc: 0.849740\n",
      "[98/100] Loss: 0.448532, Acc: 0.849826\n",
      "[98/100] Loss: 0.448621, Acc: 0.849922\n",
      "[98/100] Loss: 0.448740, Acc: 0.849417\n",
      "[98/100] Loss: 0.450747, Acc: 0.848455\n",
      "Finish 98 epochs, Loss: 0.450575, Acc: 0.848567\n",
      "Test Loss: 0.483838, Acc: 0.833966\n",
      "[99/100] Loss: 0.453119, Acc: 0.853125\n",
      "[99/100] Loss: 0.452229, Acc: 0.849583\n",
      "[99/100] Loss: 0.453551, Acc: 0.848160\n",
      "[99/100] Loss: 0.451857, Acc: 0.848203\n",
      "[99/100] Loss: 0.452184, Acc: 0.848354\n",
      "[99/100] Loss: 0.449504, Acc: 0.849201\n",
      "Finish 99 epochs, Loss: 0.450060, Acc: 0.848783\n",
      "Test Loss: 0.483397, Acc: 0.833566\n",
      "[100/100] Loss: 0.451049, Acc: 0.853646\n",
      "[100/100] Loss: 0.453461, Acc: 0.849323\n",
      "[100/100] Loss: 0.452808, Acc: 0.848889\n",
      "[100/100] Loss: 0.451040, Acc: 0.848542\n",
      "[100/100] Loss: 0.450006, Acc: 0.848938\n",
      "[100/100] Loss: 0.450757, Acc: 0.848993\n",
      "Finish 100 epochs, Loss: 0.449557, Acc: 0.849200\n",
      "Test Loss: 0.483117, Acc: 0.834565\n"
     ]
    }
   ],
   "source": [
    "# train model\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_acc = 0.0\n",
    "    for i, data in enumerate(train_loader, 1):\n",
    "        img, label = data\n",
    "        img = img.view(img.size(0), -1)\n",
    "        if use_gpu:\n",
    "            img = img.cuda()\n",
    "            label = label.cuda()\n",
    "        out = model(img)\n",
    "        loss = criterion(out, label)\n",
    "        running_loss += loss\n",
    "        _, predict = torch.max(out, 1)\n",
    "        running_acc += (predict == label).float().mean()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if i%300 == 0:\n",
    "            print(f'[{epoch+1}/{num_epochs}] Loss: {running_loss/i:.6f}, Acc: {running_acc/i:.6f}')\n",
    "    print(f'Finish {epoch+1} epochs, Loss: {running_loss/i:.6f}, Acc: {running_acc/i:.6f}')\n",
    "       \n",
    "    # test model\n",
    "    model.eval()\n",
    "    eval_loss = 0\n",
    "    eval_acc = 0\n",
    "    for data in test_loader:\n",
    "        img, label = data\n",
    "        img = img.view(img.size(0), -1)\n",
    "        if use_gpu:\n",
    "            img = img.cuda()\n",
    "            label = label.cuda()\n",
    "        with torch.no_grad():\n",
    "            out = model(img)\n",
    "            loss = criterion(out, label)\n",
    "        eval_loss += loss\n",
    "        _, predict = torch.max(out, 1)\n",
    "        eval_acc += (predict == label).float().mean()\n",
    "\n",
    "    print(f'Test Loss: {eval_loss/len(test_loader):.6f}, Acc: {eval_acc/len(test_loader):.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
